import warnings
warnings.filterwarnings("ignore")
import shutil
import os
import pandas as pd
import matplotlib
# matplotlib.use('nbAgg')
%matplotlib notebook
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pickle
from sklearn.manifold import TSNE
from sklearn import preprocessing
import pandas as pd
from multiprocessing import Process# this is used for multithreading
import multiprocessing
import codecs# this is used for file operations
import random as r
from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import log_loss
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer
from nltk import word_tokenize
from nltk.util import ngrams
import h5py
import copy
from sklearn.metrics import accuracy_score

def normalize(df):
    result1 = df.copy()
    for feature_name in df.columns:
        if (str(feature_name) != str('ID') and str(feature_name) != str('Class')):
            max_value = df[feature_name].max()
            min_value = df[feature_name].min()
            result1[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)
    return result1

def optimize_model(X_train, y_train):
    # Define the parameter grid for randomized search
    param_grid = {
        'n_neighbors': list(range(1, 21, 2)),
        'weights': ['uniform', 'distance'],
        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
    }

    # Create a KNN classifier object
    knn = KNeighborsClassifier()

    # Perform randomized search with cross-validation
    random_search = RandomizedSearchCV(knn, param_distributions=param_grid, n_iter=10, scoring='neg_log_loss', cv=5)
    random_search.fit(X_train, y_train)

    # Print the best hyperparameters found
    print("Best Hyperparameters: ", random_search.best_params_)

    # Train the KNN classifier with the best hyperparameters
    best_knn = random_search.best_estimator_
    best_knn.fit(X_train, y_train)

    return best_knn

def evaluate_model(model, X, y):
    # Predict labels
    y_pred = model.predict(X)

    # Calculate log loss and accuracy
    logloss = log_loss(y, model.predict_proba(X))
    accuracy = accuracy_score(y, y_pred)

    # Print performance metrics
    print("Log Loss: ", logloss)
    print("Accuracy: ", accuracy)

    # Plot confusion matrix
    plot_confusion_matrix(y, y_pred)

def plot_confusion_matrix(test_y, predict_y):
    C = confusion_matrix(test_y, predict_y)
    print("Number of misclassified points ", (len(test_y) - np.trace(C)) / len(test_y) * 100)

    A = (((C.T) / (C.sum(axis=1))).T)

    B = (C / C.sum(axis=0))

    labels = [1, 2, 3, 4, 5, 6, 7, 8, 9]
